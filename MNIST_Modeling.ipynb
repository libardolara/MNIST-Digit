{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification\n",
    "\n",
    "This notebook uses the PCA decomposition produced in the project to model a digit classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-procesing\n",
    "\n",
    "- Data Normalization \n",
    "- Outlier Removal\n",
    "- Feature Ranking and Selection \n",
    "- Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_26</th>\n",
       "      <th>pca_27</th>\n",
       "      <th>pca_28</th>\n",
       "      <th>pca_29</th>\n",
       "      <th>pca_30</th>\n",
       "      <th>pca_31</th>\n",
       "      <th>pca_32</th>\n",
       "      <th>pca_33</th>\n",
       "      <th>pca_34</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404.888091</td>\n",
       "      <td>274.248433</td>\n",
       "      <td>-596.804218</td>\n",
       "      <td>-398.755745</td>\n",
       "      <td>7.447199</td>\n",
       "      <td>160.094468</td>\n",
       "      <td>-219.357891</td>\n",
       "      <td>53.955521</td>\n",
       "      <td>98.801752</td>\n",
       "      <td>102.317404</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.649189</td>\n",
       "      <td>-47.003196</td>\n",
       "      <td>286.706036</td>\n",
       "      <td>36.756460</td>\n",
       "      <td>-275.819706</td>\n",
       "      <td>27.336263</td>\n",
       "      <td>78.599218</td>\n",
       "      <td>12.730552</td>\n",
       "      <td>-69.900146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-310.553121</td>\n",
       "      <td>-356.874158</td>\n",
       "      <td>43.449491</td>\n",
       "      <td>18.240244</td>\n",
       "      <td>392.227723</td>\n",
       "      <td>57.044642</td>\n",
       "      <td>-342.373177</td>\n",
       "      <td>-172.952708</td>\n",
       "      <td>26.987464</td>\n",
       "      <td>-302.480476</td>\n",
       "      <td>...</td>\n",
       "      <td>166.979171</td>\n",
       "      <td>428.325010</td>\n",
       "      <td>-399.913626</td>\n",
       "      <td>-362.429622</td>\n",
       "      <td>574.247646</td>\n",
       "      <td>140.612070</td>\n",
       "      <td>-139.009964</td>\n",
       "      <td>-100.666126</td>\n",
       "      <td>62.142199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-38.233669</td>\n",
       "      <td>-71.401235</td>\n",
       "      <td>56.230350</td>\n",
       "      <td>91.409702</td>\n",
       "      <td>-26.702068</td>\n",
       "      <td>23.986721</td>\n",
       "      <td>-126.435546</td>\n",
       "      <td>-54.152914</td>\n",
       "      <td>-22.771694</td>\n",
       "      <td>-18.233032</td>\n",
       "      <td>...</td>\n",
       "      <td>23.447529</td>\n",
       "      <td>-50.734547</td>\n",
       "      <td>192.391058</td>\n",
       "      <td>-170.791895</td>\n",
       "      <td>-119.789871</td>\n",
       "      <td>-20.553701</td>\n",
       "      <td>-103.461581</td>\n",
       "      <td>-101.097052</td>\n",
       "      <td>-15.357540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.196891</td>\n",
       "      <td>-155.751974</td>\n",
       "      <td>-35.803444</td>\n",
       "      <td>72.294971</td>\n",
       "      <td>-103.745983</td>\n",
       "      <td>-135.346679</td>\n",
       "      <td>-222.413431</td>\n",
       "      <td>109.933890</td>\n",
       "      <td>-61.898358</td>\n",
       "      <td>142.310437</td>\n",
       "      <td>...</td>\n",
       "      <td>178.425751</td>\n",
       "      <td>-126.082624</td>\n",
       "      <td>145.692001</td>\n",
       "      <td>-252.458151</td>\n",
       "      <td>-155.627146</td>\n",
       "      <td>-181.848174</td>\n",
       "      <td>219.390439</td>\n",
       "      <td>12.920194</td>\n",
       "      <td>-51.823965</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-384.347977</td>\n",
       "      <td>-288.083669</td>\n",
       "      <td>-79.374287</td>\n",
       "      <td>-34.500349</td>\n",
       "      <td>638.036772</td>\n",
       "      <td>-90.796114</td>\n",
       "      <td>-269.153581</td>\n",
       "      <td>8.267971</td>\n",
       "      <td>35.698823</td>\n",
       "      <td>-304.956709</td>\n",
       "      <td>...</td>\n",
       "      <td>69.784839</td>\n",
       "      <td>467.475721</td>\n",
       "      <td>-365.385716</td>\n",
       "      <td>-259.919098</td>\n",
       "      <td>597.024515</td>\n",
       "      <td>346.783335</td>\n",
       "      <td>-223.631365</td>\n",
       "      <td>31.981629</td>\n",
       "      <td>63.258929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pca_0       pca_1       pca_2       pca_3       pca_4       pca_5  \\\n",
       "0  404.888091  274.248433 -596.804218 -398.755745    7.447199  160.094468   \n",
       "1 -310.553121 -356.874158   43.449491   18.240244  392.227723   57.044642   \n",
       "2  -38.233669  -71.401235   56.230350   91.409702  -26.702068   23.986721   \n",
       "3   28.196891 -155.751974  -35.803444   72.294971 -103.745983 -135.346679   \n",
       "4 -384.347977 -288.083669  -79.374287  -34.500349  638.036772  -90.796114   \n",
       "\n",
       "        pca_6       pca_7      pca_8       pca_9  ...      pca_26      pca_27  \\\n",
       "0 -219.357891   53.955521  98.801752  102.317404  ...  -31.649189  -47.003196   \n",
       "1 -342.373177 -172.952708  26.987464 -302.480476  ...  166.979171  428.325010   \n",
       "2 -126.435546  -54.152914 -22.771694  -18.233032  ...   23.447529  -50.734547   \n",
       "3 -222.413431  109.933890 -61.898358  142.310437  ...  178.425751 -126.082624   \n",
       "4 -269.153581    8.267971  35.698823 -304.956709  ...   69.784839  467.475721   \n",
       "\n",
       "       pca_28      pca_29      pca_30      pca_31      pca_32      pca_33  \\\n",
       "0  286.706036   36.756460 -275.819706   27.336263   78.599218   12.730552   \n",
       "1 -399.913626 -362.429622  574.247646  140.612070 -139.009964 -100.666126   \n",
       "2  192.391058 -170.791895 -119.789871  -20.553701 -103.461581 -101.097052   \n",
       "3  145.692001 -252.458151 -155.627146 -181.848174  219.390439   12.920194   \n",
       "4 -365.385716 -259.919098  597.024515  346.783335 -223.631365   31.981629   \n",
       "\n",
       "      pca_34  label  \n",
       "0 -69.900146      1  \n",
       "1  62.142199      0  \n",
       "2 -15.357540      1  \n",
       "3 -51.823965      4  \n",
       "4  63.258929      0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the generated features data set\n",
    "\n",
    "df_features = pd.read_csv('new_features.csv')\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into 2, features and the class labels\n",
    "\n",
    "X = df_features.iloc[:,:-1]\n",
    "Y = df_features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_3</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_4</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_30</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_31</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_32</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_33</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pca_34</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-403.188901</td>\n",
       "      <td>193.677296</td>\n",
       "      <td>158.197300</td>\n",
       "      <td>395.317620</td>\n",
       "      <td>85.548006</td>\n",
       "      <td>179.670169</td>\n",
       "      <td>-2.016581</td>\n",
       "      <td>180.968981</td>\n",
       "      <td>274.594117</td>\n",
       "      <td>248.831514</td>\n",
       "      <td>...</td>\n",
       "      <td>98.081679</td>\n",
       "      <td>289.008028</td>\n",
       "      <td>112.477599</td>\n",
       "      <td>145.648132</td>\n",
       "      <td>-2.052108</td>\n",
       "      <td>249.298071</td>\n",
       "      <td>44.281561</td>\n",
       "      <td>149.299872</td>\n",
       "      <td>38.090874</td>\n",
       "      <td>159.314530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128.312444</td>\n",
       "      <td>253.860200</td>\n",
       "      <td>82.056237</td>\n",
       "      <td>162.753303</td>\n",
       "      <td>-95.837773</td>\n",
       "      <td>165.305724</td>\n",
       "      <td>-29.978093</td>\n",
       "      <td>173.646222</td>\n",
       "      <td>14.681545</td>\n",
       "      <td>93.042506</td>\n",
       "      <td>...</td>\n",
       "      <td>-183.614473</td>\n",
       "      <td>94.177847</td>\n",
       "      <td>-47.012869</td>\n",
       "      <td>62.471501</td>\n",
       "      <td>-0.613047</td>\n",
       "      <td>83.333289</td>\n",
       "      <td>-22.923784</td>\n",
       "      <td>60.445855</td>\n",
       "      <td>-36.246536</td>\n",
       "      <td>55.936291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>292.279563</td>\n",
       "      <td>266.873642</td>\n",
       "      <td>-0.190937</td>\n",
       "      <td>236.567255</td>\n",
       "      <td>-97.099290</td>\n",
       "      <td>186.784327</td>\n",
       "      <td>-15.962904</td>\n",
       "      <td>257.264589</td>\n",
       "      <td>-3.362085</td>\n",
       "      <td>185.811530</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.774060</td>\n",
       "      <td>240.115722</td>\n",
       "      <td>89.179964</td>\n",
       "      <td>210.915683</td>\n",
       "      <td>35.371027</td>\n",
       "      <td>248.483410</td>\n",
       "      <td>-62.793926</td>\n",
       "      <td>193.251075</td>\n",
       "      <td>-67.990988</td>\n",
       "      <td>165.213303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-146.522003</td>\n",
       "      <td>256.414307</td>\n",
       "      <td>-2.946650</td>\n",
       "      <td>216.411671</td>\n",
       "      <td>-340.873684</td>\n",
       "      <td>160.903833</td>\n",
       "      <td>92.675048</td>\n",
       "      <td>160.841310</td>\n",
       "      <td>-40.872873</td>\n",
       "      <td>147.602185</td>\n",
       "      <td>...</td>\n",
       "      <td>61.064153</td>\n",
       "      <td>239.744332</td>\n",
       "      <td>41.462769</td>\n",
       "      <td>213.126708</td>\n",
       "      <td>116.984779</td>\n",
       "      <td>218.795619</td>\n",
       "      <td>-69.408991</td>\n",
       "      <td>183.827842</td>\n",
       "      <td>-56.940501</td>\n",
       "      <td>260.045084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.727567</td>\n",
       "      <td>239.844450</td>\n",
       "      <td>-43.107924</td>\n",
       "      <td>222.453282</td>\n",
       "      <td>160.663887</td>\n",
       "      <td>192.244807</td>\n",
       "      <td>-44.090419</td>\n",
       "      <td>164.709084</td>\n",
       "      <td>-149.231415</td>\n",
       "      <td>148.678630</td>\n",
       "      <td>...</td>\n",
       "      <td>19.521373</td>\n",
       "      <td>177.829332</td>\n",
       "      <td>140.012218</td>\n",
       "      <td>190.282084</td>\n",
       "      <td>8.738864</td>\n",
       "      <td>134.492993</td>\n",
       "      <td>80.704519</td>\n",
       "      <td>173.674909</td>\n",
       "      <td>52.171234</td>\n",
       "      <td>112.464246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-201.323276</td>\n",
       "      <td>249.888237</td>\n",
       "      <td>75.892370</td>\n",
       "      <td>293.375768</td>\n",
       "      <td>-53.177698</td>\n",
       "      <td>174.932873</td>\n",
       "      <td>-31.315788</td>\n",
       "      <td>158.185820</td>\n",
       "      <td>-6.667390</td>\n",
       "      <td>128.868084</td>\n",
       "      <td>...</td>\n",
       "      <td>-101.839382</td>\n",
       "      <td>245.926549</td>\n",
       "      <td>-38.912705</td>\n",
       "      <td>309.952217</td>\n",
       "      <td>21.637704</td>\n",
       "      <td>231.969999</td>\n",
       "      <td>-68.840978</td>\n",
       "      <td>265.880189</td>\n",
       "      <td>97.535498</td>\n",
       "      <td>197.015295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-88.351114</td>\n",
       "      <td>241.520418</td>\n",
       "      <td>-46.994364</td>\n",
       "      <td>360.826148</td>\n",
       "      <td>287.476270</td>\n",
       "      <td>217.818060</td>\n",
       "      <td>-45.793166</td>\n",
       "      <td>187.241643</td>\n",
       "      <td>-211.684515</td>\n",
       "      <td>181.072208</td>\n",
       "      <td>...</td>\n",
       "      <td>112.264046</td>\n",
       "      <td>238.841266</td>\n",
       "      <td>-162.377972</td>\n",
       "      <td>137.861167</td>\n",
       "      <td>-78.701276</td>\n",
       "      <td>202.287687</td>\n",
       "      <td>-6.263748</td>\n",
       "      <td>176.686162</td>\n",
       "      <td>43.579690</td>\n",
       "      <td>203.405775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.498699</td>\n",
       "      <td>297.831952</td>\n",
       "      <td>-172.040498</td>\n",
       "      <td>153.064324</td>\n",
       "      <td>-24.015339</td>\n",
       "      <td>150.328630</td>\n",
       "      <td>279.407474</td>\n",
       "      <td>247.903740</td>\n",
       "      <td>95.777371</td>\n",
       "      <td>177.128097</td>\n",
       "      <td>...</td>\n",
       "      <td>23.934348</td>\n",
       "      <td>139.738407</td>\n",
       "      <td>-68.942322</td>\n",
       "      <td>165.267984</td>\n",
       "      <td>-94.769463</td>\n",
       "      <td>179.618171</td>\n",
       "      <td>-31.944567</td>\n",
       "      <td>206.077637</td>\n",
       "      <td>-40.991437</td>\n",
       "      <td>154.776844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>195.296177</td>\n",
       "      <td>269.761801</td>\n",
       "      <td>26.399690</td>\n",
       "      <td>286.669941</td>\n",
       "      <td>-13.681017</td>\n",
       "      <td>213.795448</td>\n",
       "      <td>-184.707868</td>\n",
       "      <td>196.873362</td>\n",
       "      <td>28.806149</td>\n",
       "      <td>165.783492</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.499150</td>\n",
       "      <td>223.553401</td>\n",
       "      <td>-16.602025</td>\n",
       "      <td>167.305283</td>\n",
       "      <td>19.490204</td>\n",
       "      <td>174.587963</td>\n",
       "      <td>173.488073</td>\n",
       "      <td>169.065498</td>\n",
       "      <td>-2.389740</td>\n",
       "      <td>224.188354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-20.532851</td>\n",
       "      <td>202.411289</td>\n",
       "      <td>-81.047079</td>\n",
       "      <td>237.809149</td>\n",
       "      <td>97.068540</td>\n",
       "      <td>170.576752</td>\n",
       "      <td>-25.078020</td>\n",
       "      <td>214.749183</td>\n",
       "      <td>-2.206619</td>\n",
       "      <td>133.258879</td>\n",
       "      <td>...</td>\n",
       "      <td>51.382085</td>\n",
       "      <td>169.127950</td>\n",
       "      <td>-41.382253</td>\n",
       "      <td>207.752484</td>\n",
       "      <td>-19.088334</td>\n",
       "      <td>161.202266</td>\n",
       "      <td>-23.246896</td>\n",
       "      <td>172.406229</td>\n",
       "      <td>-11.381110</td>\n",
       "      <td>128.464298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pca_0                   pca_1                   pca_2              \\\n",
       "             mean         std        mean         std        mean         std   \n",
       "label                                                                           \n",
       "0     -403.188901  193.677296  158.197300  395.317620   85.548006  179.670169   \n",
       "1      128.312444  253.860200   82.056237  162.753303  -95.837773  165.305724   \n",
       "2      292.279563  266.873642   -0.190937  236.567255  -97.099290  186.784327   \n",
       "3     -146.522003  256.414307   -2.946650  216.411671 -340.873684  160.903833   \n",
       "4      120.727567  239.844450  -43.107924  222.453282  160.663887  192.244807   \n",
       "5     -201.323276  249.888237   75.892370  293.375768  -53.177698  174.932873   \n",
       "6      -88.351114  241.520418  -46.994364  360.826148  287.476270  217.818060   \n",
       "7       74.498699  297.831952 -172.040498  153.064324  -24.015339  150.328630   \n",
       "8      195.296177  269.761801   26.399690  286.669941  -13.681017  213.795448   \n",
       "9      -20.532851  202.411289  -81.047079  237.809149   97.068540  170.576752   \n",
       "\n",
       "            pca_3                   pca_4              ...      pca_30  \\\n",
       "             mean         std        mean         std  ...        mean   \n",
       "label                                                  ...               \n",
       "0       -2.016581  180.968981  274.594117  248.831514  ...   98.081679   \n",
       "1      -29.978093  173.646222   14.681545   93.042506  ... -183.614473   \n",
       "2      -15.962904  257.264589   -3.362085  185.811530  ...  -39.774060   \n",
       "3       92.675048  160.841310  -40.872873  147.602185  ...   61.064153   \n",
       "4      -44.090419  164.709084 -149.231415  148.678630  ...   19.521373   \n",
       "5      -31.315788  158.185820   -6.667390  128.868084  ... -101.839382   \n",
       "6      -45.793166  187.241643 -211.684515  181.072208  ...  112.264046   \n",
       "7      279.407474  247.903740   95.777371  177.128097  ...   23.934348   \n",
       "8     -184.707868  196.873362   28.806149  165.783492  ...  -20.499150   \n",
       "9      -25.078020  214.749183   -2.206619  133.258879  ...   51.382085   \n",
       "\n",
       "                       pca_31                  pca_32                  pca_33  \\\n",
       "              std        mean         std        mean         std        mean   \n",
       "label                                                                           \n",
       "0      289.008028  112.477599  145.648132   -2.052108  249.298071   44.281561   \n",
       "1       94.177847  -47.012869   62.471501   -0.613047   83.333289  -22.923784   \n",
       "2      240.115722   89.179964  210.915683   35.371027  248.483410  -62.793926   \n",
       "3      239.744332   41.462769  213.126708  116.984779  218.795619  -69.408991   \n",
       "4      177.829332  140.012218  190.282084    8.738864  134.492993   80.704519   \n",
       "5      245.926549  -38.912705  309.952217   21.637704  231.969999  -68.840978   \n",
       "6      238.841266 -162.377972  137.861167  -78.701276  202.287687   -6.263748   \n",
       "7      139.738407  -68.942322  165.267984  -94.769463  179.618171  -31.944567   \n",
       "8      223.553401  -16.602025  167.305283   19.490204  174.587963  173.488073   \n",
       "9      169.127950  -41.382253  207.752484  -19.088334  161.202266  -23.246896   \n",
       "\n",
       "                      pca_34              \n",
       "              std       mean         std  \n",
       "label                                     \n",
       "0      149.299872  38.090874  159.314530  \n",
       "1       60.445855 -36.246536   55.936291  \n",
       "2      193.251075 -67.990988  165.213303  \n",
       "3      183.827842 -56.940501  260.045084  \n",
       "4      173.674909  52.171234  112.464246  \n",
       "5      265.880189  97.535498  197.015295  \n",
       "6      176.686162  43.579690  203.405775  \n",
       "7      206.077637 -40.991437  154.776844  \n",
       "8      169.065498  -2.389740  224.188354  \n",
       "9      172.406229 -11.381110  128.464298  \n",
       "\n",
       "[10 rows x 70 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with Feature ranking and selection\n",
    "# Calculate the mean, trimmed mean, and standard deviation per species\n",
    "\n",
    "df_mean = df_features.groupby('label').agg(['mean', 'std'])\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_25</th>\n",
       "      <th>pca_26</th>\n",
       "      <th>pca_27</th>\n",
       "      <th>pca_28</th>\n",
       "      <th>pca_29</th>\n",
       "      <th>pca_30</th>\n",
       "      <th>pca_31</th>\n",
       "      <th>pca_32</th>\n",
       "      <th>pca_33</th>\n",
       "      <th>pca_34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0v1</th>\n",
       "      <td>2.770744</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.551959</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.957215</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.051145</td>\n",
       "      <td>0.162363</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.049055</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040487</td>\n",
       "      <td>0.019034</td>\n",
       "      <td>2.209417</td>\n",
       "      <td>1.290776</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.858841</td>\n",
       "      <td>1.012786</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.174088</td>\n",
       "      <td>0.193829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v2</th>\n",
       "      <td>4.448317</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.496653</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.801091</td>\n",
       "      <td>0.090427</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.078045</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>0.048837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183807</td>\n",
       "      <td>1.300087</td>\n",
       "      <td>1.043983</td>\n",
       "      <td>0.068906</td>\n",
       "      <td>0.394624</td>\n",
       "      <td>0.134609</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.192251</td>\n",
       "      <td>0.213631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v3</th>\n",
       "      <td>0.637986</td>\n",
       "      <td>0.127849</td>\n",
       "      <td>3.125856</td>\n",
       "      <td>0.152961</td>\n",
       "      <td>1.188951</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.025028</td>\n",
       "      <td>0.070329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183540</td>\n",
       "      <td>0.325633</td>\n",
       "      <td>1.875118</td>\n",
       "      <td>0.165606</td>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.075681</td>\n",
       "      <td>0.128792</td>\n",
       "      <td>0.230471</td>\n",
       "      <td>0.097102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v4</th>\n",
       "      <td>2.888250</td>\n",
       "      <td>0.196945</td>\n",
       "      <td>0.081491</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>2.137858</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>0.778098</td>\n",
       "      <td>0.015935</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>...</td>\n",
       "      <td>3.822427</td>\n",
       "      <td>0.080108</td>\n",
       "      <td>4.097299</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.121315</td>\n",
       "      <td>0.053598</td>\n",
       "      <td>0.013204</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v5</th>\n",
       "      <td>0.407681</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.306042</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>1.007436</td>\n",
       "      <td>0.076015</td>\n",
       "      <td>0.415182</td>\n",
       "      <td>0.331016</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.450868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024580</td>\n",
       "      <td>0.334742</td>\n",
       "      <td>0.543892</td>\n",
       "      <td>0.099496</td>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.277548</td>\n",
       "      <td>0.195415</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.055045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pca_0     pca_1     pca_2     pca_3     pca_4     pca_5     pca_6  \\\n",
       "0v1  2.770744  0.031721  0.551959  0.012429  0.957215  0.004209  0.051145   \n",
       "0v2  4.448317  0.118200  0.496653  0.001966  0.801091  0.090427  0.012974   \n",
       "0v3  0.637986  0.127849  3.125856  0.152961  1.188951  0.024472  0.481980   \n",
       "0v4  2.888250  0.196945  0.081491  0.029563  2.137858  0.005444  0.301530   \n",
       "0v5  0.407681  0.027952  0.306042  0.014859  1.007436  0.076015  0.415182   \n",
       "\n",
       "        pca_7     pca_8     pca_9  ...    pca_25    pca_26    pca_27  \\\n",
       "0v1  0.162363  0.014710  0.049055  ...  1.040487  0.019034  2.209417   \n",
       "0v2  0.078045  0.049902  0.048837  ...  0.183807  1.300087  1.043983   \n",
       "0v3  0.005534  0.025028  0.070329  ...  0.183540  0.325633  1.875118   \n",
       "0v4  0.778098  0.015935  0.121681  ...  3.822427  0.080108  4.097299   \n",
       "0v5  0.331016  0.023798  0.450868  ...  0.024580  0.334742  0.543892   \n",
       "\n",
       "       pca_28    pca_29    pca_30    pca_31    pca_32    pca_33    pca_34  \n",
       "0v1  1.290776  0.165390  0.858841  1.012786  0.000030  0.174088  0.193829  \n",
       "0v2  0.068906  0.394624  0.134609  0.008262  0.011304  0.192251  0.213631  \n",
       "0v3  0.165606  0.008697  0.009718  0.075681  0.128792  0.230471  0.097102  \n",
       "0v4  0.062533  0.121315  0.053598  0.013204  0.001451  0.025292  0.005213  \n",
       "0v5  0.099496  0.062397  0.277548  0.195415  0.004840  0.137625  0.055045  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "# Use the FDR in a one vs one analysis to select features that contribute the most to class separation. \n",
    "# Get all combinations of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] and length 2\n",
    "\n",
    "versus = combinations([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 2)\n",
    "df_fdr = pd.DataFrame(columns=X.columns)\n",
    "for i,j in versus:\n",
    "    # Calculating the FDR of each pca feature i vs j\n",
    "    row = []\n",
    "    for feature in X.columns:\n",
    "        mu_i = df_mean[feature]['mean'][i]\n",
    "        mu_j = df_mean[feature]['mean'][j]\n",
    "        var_i = df_mean[feature]['std'][i]**2\n",
    "        var_j = df_mean[feature]['std'][j]**2\n",
    "        fdr_i_j = (mu_i - mu_j)**2 / (var_i + var_j)\n",
    "        row.append(fdr_i_j)\n",
    "    df_fdr.loc[f'{i}v{j}'] = row\n",
    "df_fdr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features kept\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_11</th>\n",
       "      <th>pca_15</th>\n",
       "      <th>pca_16</th>\n",
       "      <th>pca_17</th>\n",
       "      <th>pca_18</th>\n",
       "      <th>pca_25</th>\n",
       "      <th>pca_26</th>\n",
       "      <th>pca_27</th>\n",
       "      <th>pca_28</th>\n",
       "      <th>pca_29</th>\n",
       "      <th>pca_30</th>\n",
       "      <th>pca_31</th>\n",
       "      <th>pca_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0v1</th>\n",
       "      <td>2.770744</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.551959</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.957215</td>\n",
       "      <td>0.051145</td>\n",
       "      <td>0.162363</td>\n",
       "      <td>0.060776</td>\n",
       "      <td>15.227147</td>\n",
       "      <td>0.011076</td>\n",
       "      <td>0.368115</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>1.040487</td>\n",
       "      <td>0.019034</td>\n",
       "      <td>2.209417</td>\n",
       "      <td>1.290776</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.858841</td>\n",
       "      <td>1.012786</td>\n",
       "      <td>0.174088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v2</th>\n",
       "      <td>4.448317</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.496653</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.801091</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.078045</td>\n",
       "      <td>0.643916</td>\n",
       "      <td>2.317963</td>\n",
       "      <td>0.235194</td>\n",
       "      <td>0.295963</td>\n",
       "      <td>0.319519</td>\n",
       "      <td>0.183807</td>\n",
       "      <td>1.300087</td>\n",
       "      <td>1.043983</td>\n",
       "      <td>0.068906</td>\n",
       "      <td>0.394624</td>\n",
       "      <td>0.134609</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.192251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v3</th>\n",
       "      <td>0.637986</td>\n",
       "      <td>0.127849</td>\n",
       "      <td>3.125856</td>\n",
       "      <td>0.152961</td>\n",
       "      <td>1.188951</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.104204</td>\n",
       "      <td>4.562296</td>\n",
       "      <td>0.435353</td>\n",
       "      <td>0.477843</td>\n",
       "      <td>0.643963</td>\n",
       "      <td>0.183540</td>\n",
       "      <td>0.325633</td>\n",
       "      <td>1.875118</td>\n",
       "      <td>0.165606</td>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.075681</td>\n",
       "      <td>0.230471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v4</th>\n",
       "      <td>2.888250</td>\n",
       "      <td>0.196945</td>\n",
       "      <td>0.081491</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>2.137858</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>0.778098</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.894532</td>\n",
       "      <td>0.971207</td>\n",
       "      <td>0.626631</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>3.822427</td>\n",
       "      <td>0.080108</td>\n",
       "      <td>4.097299</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.121315</td>\n",
       "      <td>0.053598</td>\n",
       "      <td>0.013204</td>\n",
       "      <td>0.025292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0v5</th>\n",
       "      <td>0.407681</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.306042</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>1.007436</td>\n",
       "      <td>0.415182</td>\n",
       "      <td>0.331016</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>3.145986</td>\n",
       "      <td>0.562648</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.090093</td>\n",
       "      <td>0.024580</td>\n",
       "      <td>0.334742</td>\n",
       "      <td>0.543892</td>\n",
       "      <td>0.099496</td>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.277548</td>\n",
       "      <td>0.195415</td>\n",
       "      <td>0.137625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pca_0     pca_1     pca_2     pca_3     pca_4     pca_6     pca_7  \\\n",
       "0v1  2.770744  0.031721  0.551959  0.012429  0.957215  0.051145  0.162363   \n",
       "0v2  4.448317  0.118200  0.496653  0.001966  0.801091  0.012974  0.078045   \n",
       "0v3  0.637986  0.127849  3.125856  0.152961  1.188951  0.481980  0.005534   \n",
       "0v4  2.888250  0.196945  0.081491  0.029563  2.137858  0.301530  0.778098   \n",
       "0v5  0.407681  0.027952  0.306042  0.014859  1.007436  0.415182  0.331016   \n",
       "\n",
       "       pca_11     pca_15    pca_16    pca_17    pca_18    pca_25    pca_26  \\\n",
       "0v1  0.060776  15.227147  0.011076  0.368115  0.006641  1.040487  0.019034   \n",
       "0v2  0.643916   2.317963  0.235194  0.295963  0.319519  0.183807  1.300087   \n",
       "0v3  0.104204   4.562296  0.435353  0.477843  0.643963  0.183540  0.325633   \n",
       "0v4  0.039380   0.894532  0.971207  0.626631  0.006211  3.822427  0.080108   \n",
       "0v5  0.006673   3.145986  0.562648  0.014054  0.090093  0.024580  0.334742   \n",
       "\n",
       "       pca_27    pca_28    pca_29    pca_30    pca_31    pca_33  \n",
       "0v1  2.209417  1.290776  0.165390  0.858841  1.012786  0.174088  \n",
       "0v2  1.043983  0.068906  0.394624  0.134609  0.008262  0.192251  \n",
       "0v3  1.875118  0.165606  0.008697  0.009718  0.075681  0.230471  \n",
       "0v4  4.097299  0.062533  0.121315  0.053598  0.013204  0.025292  \n",
       "0v5  0.543892  0.099496  0.062397  0.277548  0.195415  0.137625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial analysis of the features that bring the most separation\n",
    "dmin = min(df_fdr.max(axis=1)) # Takes the max values from every versus and from those take the minimum value (Weakest versus FDR)\n",
    "cols = [f'pca_{x}'  for x in range(0, 35) if len(df_fdr[df_fdr[f'pca_{x}']>=dmin]) !=0]  # Only taking features with separation greatest than found in the weakest versus\n",
    "print(len(cols), 'features kept')\n",
    "\n",
    "# Thease are the top features\n",
    "df_fdr_top = df_fdr.filter(items=cols)\n",
    "df_fdr_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca_26    3.926725\n",
       "pca_15    3.830812\n",
       "pca_25    2.739670\n",
       "pca_2     2.043277\n",
       "pca_27    1.808842\n",
       "pca_0     1.800655\n",
       "pca_7     1.350468\n",
       "pca_16    1.338104\n",
       "pca_29    1.331732\n",
       "pca_28    1.312715\n",
       "pca_1     1.293442\n",
       "pca_31    1.283205\n",
       "pca_18    1.246955\n",
       "pca_4     1.241101\n",
       "pca_11    1.194765\n",
       "pca_3     1.185107\n",
       "pca_30    1.183169\n",
       "pca_17    1.176178\n",
       "pca_6     1.085156\n",
       "pca_33    0.996454\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking the feature kept from the previous step\n",
    "# Using only the siginificant values (greater than the weakest versus) to calculate the mean per feature\n",
    "df_fdr_top_ranked = df_fdr_top[df_fdr_top >= dmin].mean().sort_values(ascending=False)\n",
    "df_fdr_top_ranked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with 35 features, then selected the 20 top features with significant separability per Fisher's linear discriminant. Then I ranked the 20 features by how much useful information (disregarding low digit separabilities) it would add to separate features. \n",
    "\n",
    "**PCA_26**, **PCA_15**, **PCA_25**, **PCA_2**, and **PCA_27** are the top features.**PCA_25** and **PCA_26** are the first features from the horizontal coefficients. Again, we could divide numbers that could have a horizontal component (1,2,3,4,5,7) and those who don't (0,8,9) **PCA_15** is the first feature from the vertical coefficients, it contains the maximum variance from all the information acquired from verticality. This makes sense since we can divide numbers that could have a vertical component (1,4,5,7,9) and those who don't (0,2,3,8) and most likely **PCA_15** is making use of this fact. \n",
    "\n",
    "> Next I'll remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the top ranked features\n",
    "X_filtered = X.filter(items=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(data, col):\n",
    "  '''\n",
    "    Function to calculate the squared Mahalanobis Distance.\n",
    "    data:   A pandas data frame with the features to calculate the MD2\n",
    "    Returns the MD^2 = (x-mu)*inv(cov)*(x-mu)^T\n",
    "  '''\n",
    "  x = np.array(data)\n",
    "  x_mu = x - x.mean(axis=0)\n",
    "  cov = np.cov(x.T)\n",
    "  inv_cov = np.linalg.inv(cov)\n",
    "  left = np.dot(x_mu, inv_cov)\n",
    "  mahal = np.dot(left, x_mu.T)\n",
    "  mahal = mahal.diagonal()\n",
    "  return pd.DataFrame(mahal, index=data.index, columns=[col]) # Returns the distance with the same indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier removal using a Wilks outlier removal\n",
    "# Using the Mahalanobis distance to then remove those with a distance statistically significance\n",
    "# Implemented the Wilks outlier removal proposed with the Bonferroni correction\n",
    "\n",
    "md2 = pd.DataFrame(columns=['md2'])\n",
    "for c in range(0,10):\n",
    "  md2 = pd.concat([md2, mahalanobis((X_filtered[Y==c]), 'md2')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed index order and data types\n",
    "md2 = md2.sort_index()\n",
    "md2 = md2.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>345.849609</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>260.171792</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>228.685975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>211.186562</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>191.841780</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>4.400966</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>4.378405</td>\n",
       "      <td>0.999788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>4.370533</td>\n",
       "      <td>0.999791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>4.363343</td>\n",
       "      <td>0.999793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>4.045544</td>\n",
       "      <td>0.999884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             md2         p\n",
       "4789  345.849609  0.000000\n",
       "622   260.171792  0.000000\n",
       "191   228.685975  0.000000\n",
       "2284  211.186562  0.000000\n",
       "4950  191.841780  0.000000\n",
       "...          ...       ...\n",
       "2942    4.400966  0.999779\n",
       "3423    4.378405  0.999788\n",
       "3584    4.370533  0.999791\n",
       "4394    4.363343  0.999793\n",
       "2852    4.045544  0.999884\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "# Using a Chi2 test to determine the statistical significance of the md2\n",
    "\n",
    "md2['p'] = 1 - stats.chi2.cdf(md2['md2'], len(cols)-1) # Using k-1 degrees of freedom.\n",
    "md2.sort_values(by='md2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting the MD2 by class\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = plt.subplot()\n",
    "for i in range(0, 10):\n",
    "    plt.scatter(range(1, 1 + len(md2[Y==i])), md2[Y==i].sort_values(by='md2', ascending=True)['md2'], label=i)\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel('MD2')\n",
    "    plt.legend()\n",
    "ax.axhline(y=md2[md2['p'] < 0.003]['md2'].min(), color=\"red\")\n",
    "ax.text(100,50, \"p 0.003\", color=\"red\",  ha=\"right\", va=\"center\")\n",
    "plt.title('Mahalnobis Distance Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using p 0.003 as the threshold for outliers\n",
    "\n",
    "X_train = X_filtered[md2['p'] >= 0.003]\n",
    "y_train = Y[md2['p'] >= 0.003]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers I will attempt to prove that all features add to increase the ability to classify the 9 digits. To do so, I will use a support vector machine as base model and a pipeline to normalize and train the models within a 5-fold cross validation. Adding features every iteration, from higher FDR to lower, expecting that each combination will increase the accuracy of the model. If a combination does not increase the accuracy the last feature is dropped. At the end we would expect a better accuracy than if we were to use all features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sklearn built in function to split data into 5 folds\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('sc', StandardScaler()),\n",
    "            ('classifier', SVC(kernel = 'rbf'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.86 %\n",
      "Standard Deviation: 0.00 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-Fold Cross Validation using all the top ranked features\n",
    "accuracies = cross_val_score(estimator = pipe, X = X_train, y = y_train, cv = 5)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} \".format(accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.39 %\n",
      "Accuracy: 45.20 %\n",
      "Accuracy: 65.15 %\n",
      "Accuracy: 72.00 %\n",
      "Accuracy: 80.01 %\n",
      "Accuracy: 84.83 %\n",
      "Accuracy: 86.33 %\n",
      "Accuracy: 87.53 %\n",
      "Accuracy: 89.77 %\n",
      "Accuracy: 91.53 %\n",
      "Accuracy: 93.07 %\n",
      "Accuracy: 93.47 %\n",
      "Accuracy: 94.36 %\n",
      "Accuracy: 94.82 %\n",
      "Accuracy: 94.61 %\n",
      "Dropped pca_11\n",
      "Accuracy: 94.99 %\n",
      "Accuracy: 95.35 %\n",
      "Accuracy: 95.54 %\n",
      "Accuracy: 95.84 %\n",
      "Accuracy: 95.98 %\n",
      "['pca_26', 'pca_15', 'pca_25', 'pca_2', 'pca_27', 'pca_0', 'pca_7', 'pca_16', 'pca_29', 'pca_28', 'pca_1', 'pca_31', 'pca_18', 'pca_4', 'pca_3', 'pca_30', 'pca_17', 'pca_6', 'pca_33']\n",
      "Best accuracy: 95.98 %\n"
     ]
    }
   ],
   "source": [
    "# Foward selection of the top ranked features\n",
    "current_acc = 0\n",
    "c_tested  = []\n",
    "for c in df_fdr_top_ranked.index:\n",
    "    c_tested.append(c)\n",
    "    X_f = X_train.filter(items=c_tested)\n",
    "    accuracies = cross_val_score(estimator = pipe, X = X_f, y = y_train, cv = 5)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    if(accuracies.mean()>=current_acc):\n",
    "        current_acc = accuracies.mean()\n",
    "    else:\n",
    "        print(\"Dropped\", c)\n",
    "        c_tested.pop()\n",
    "\n",
    "print(c_tested)\n",
    "print(\"Best accuracy: {:.2f} %\".format(current_acc*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "- Support Vector Machine (kernel of your choice)\n",
    "- Random Forest\n",
    "- Feed Foward NN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will implement a Suppor Vector Machine, Random Forest and a FFNN to test the best model. Using GridSearchCV I will do the 5-fold cross validation and at the same time I will test different hyperparameters looking for the best combination. For the SVM I will tune C, Kernel and Gamma hyperparameter. For the Random Forest I will tune the ammount of estimators and the criterion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Methodology \n",
    "I will use 5-fold cross validation with each of the models chosen. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 96.28 %\n",
      "Best Parameters: {'classifier__C': 1, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluate if it was better to use all top ranked features or a special combination\n",
    "if accuracies.mean() <= current_acc:\n",
    "    X_train = X_train.filter(items=c_tested)\n",
    "\n",
    "parameters = [{'classifier__C': [0.25, 0.5, 0.75, 1], 'classifier__kernel': ['poly', 'rbf', 'sigmoid',], 'classifier__gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = pipe,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe2 = Pipeline([\n",
    "            ('sc', StandardScaler()),\n",
    "            ('classifier', RandomForestClassifier(random_state = 0))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 92.73 %\n",
      "Best Parameters: {'classifier__criterion': 'entropy', 'classifier__n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'classifier__n_estimators': [100, 300, 400, 500, 1000], 'classifier__criterion': ['entropy', 'gini']}]\n",
    "grid_search = GridSearchCV(estimator = pipe2,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Foward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(hidden_layer_dim):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu', input_shape=(19,)))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "            ('sc', StandardScaler()),\n",
    "            ('classifier', KerasClassifier(model=create_model, verbose=0, hidden_layer_dim=100))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"classifier__batch_size\": [32, 64],\n",
    "    \"classifier__epochs\": [100],\n",
    "    \"classifier__hidden_layer_dim\": [300],\n",
    "    \"classifier__loss\": [\"categorical_crossentropy\"],\n",
    "    \"classifier__optimizer\": [\"adam\"],\n",
    "    \"classifier__optimizer__learning_rate\": [0.001],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4732 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9\n",
       "0     0  1  0  0  0  0  0  0  0  0\n",
       "1     1  0  0  0  0  0  0  0  0  0\n",
       "2     0  1  0  0  0  0  0  0  0  0\n",
       "3     0  0  0  0  1  0  0  0  0  0\n",
       "4     1  0  0  0  0  0  0  0  0  0\n",
       "...  .. .. .. .. .. .. .. .. .. ..\n",
       "4995  0  0  1  0  0  0  0  0  0  0\n",
       "4996  0  0  0  0  0  1  0  0  0  0\n",
       "4997  0  0  0  0  0  0  0  0  1  0\n",
       "4998  0  0  0  0  0  0  0  1  0  0\n",
       "4999  0  0  0  0  0  0  0  0  0  1\n",
       "\n",
       "[4732 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y = pd.get_dummies(y_train)\n",
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 23:42:08.113517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113771: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113804: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.113811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.114193: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.114515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:42:08.115092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 23:44:32.336405: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.961537 using {'classifier__batch_size': 64, 'classifier__epochs': 100, 'classifier__hidden_layer_dim': 300, 'classifier__loss': 'categorical_crossentropy', 'classifier__optimizer': 'adam', 'classifier__optimizer__learning_rate': 0.001}\n",
      "0.961115 (0.005731) with: {'classifier__batch_size': 32, 'classifier__epochs': 100, 'classifier__hidden_layer_dim': 300, 'classifier__loss': 'categorical_crossentropy', 'classifier__optimizer': 'adam', 'classifier__optimizer__learning_rate': 0.001}\n",
      "0.961537 (0.005785) with: {'classifier__batch_size': 64, 'classifier__epochs': 100, 'classifier__hidden_layer_dim': 300, 'classifier__loss': 'categorical_crossentropy', 'classifier__optimizer': 'adam', 'classifier__optimizer__learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe3, param_grid=parameters, n_jobs=-1, cv=5)\n",
    "grid_result = grid.fit(X_train, dummy_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination I found was using FDR for feature selection and ranking combined with Mahalanobis distance and standardized normalization. I ran several analyses with the threshold on Mahalanobis distance and the best value was after the elbow from the graph x, at a distance of 40 or a p-value of 0.003. A higher threshold meant that noisy observations were introduced into the 5-fold cross-validation, affecting the accuracy. The best model was the Feed Foward Neural network with 5 hidden layers. The SVM performed extremely well too, with the advantage of lower computational demand. I also tried using the DCT absolute values but the result was significantly worst. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the FDR, Mahalanobis distance, and Standard normalization because they gave a sound mathematical background. FDR is allowing me to analyze how each feature performs to separate one class from the others. With FDR I was able to rank and select features, reducing by 10 the dimensionality. Mahalanobis distance gives an actual distance in multivariate space, to which I could analyze the outlier. My graph showed how the observations formed an L shape, having an inflection point where the observations exponentially increased the distance. That point was a sound indication of outliers, accounting for a population that was tightly close and another that was separated. \n",
    "\n",
    "I chose to test SVM and Random Forest because these were models I've implemented in the past with good results in classification problems. I was interested to see if the FFNN would be able to beat the SVM model with the computing resources at my disposal. Although it did it wasn't easy, I had to run many iterations to find a good architecture of the NN and hyperparameters that would actually be better than the SVM. The difference in accuracy is slight but the compute resource needed was astronomically greater for the NN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5-fold cross-validation with FFNN was very taxing. The combination of testing different hyperparameters and doing the 5-fold was too high of a computational demand for a neural network. I was trying different batch sizes and epochs, while at the same time testing different units per hidden layer. Basically, every hyperparameter, the 5-fold and the epochs could be added to a big O notations of O(n^b) with b in the orders of 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
